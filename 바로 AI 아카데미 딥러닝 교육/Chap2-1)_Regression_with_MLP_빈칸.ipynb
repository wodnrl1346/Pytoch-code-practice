{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP(Multi Layer Perceptron) - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#데이터 판다스로 불러오기\n",
    "df_raw = pd.read_csv(\"./BostonHousing.csv\") #csv(comma-separated values)파일 가져오기(디럭토리 주소입력)\n",
    "print(type(df_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
       "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
       "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
       "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n",
       "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n",
       "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n",
       "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n",
       "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n",
       "\n",
       "     ptratio   black  lstat  medv  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_raw.shape) # Dataset의 크기 확인\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# X값 coulumn\n",
    "x = df_raw.drop([\"medv\"], axis=1) #Target 값에 해당하는 집값('medv') 열을 삭제\n",
    "\n",
    "# y값 column\n",
    "y = df_raw[\"medv\"]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train의 크기:  (354, 13)\n",
      "y_train의 크기:  (354,) \n",
      "\n",
      "x_test의 크기:  (152, 13)\n",
      "y_test의 크기:  (152,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#학습데이터와 테스트데이터를 일정비율로 나누기\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "#학습 데이터\n",
    "print(\"x_train의 크기: \",x_train.shape)\n",
    "print(\"y_train의 크기: \",y_train.shape,'\\n')\n",
    "\n",
    "#테스트 데이터 \n",
    "print(\"x_test의 크기: \",x_test.shape)\n",
    "print(\"y_test의 크기: \",y_test.shape)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -Scailing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 데이터 Scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scale = scaler.transform(x_train) # x_train_scale은 numpy ndarray \n",
    "\n",
    "\n",
    "#테스트 데이터 Scaling\n",
    "x_test_scale = scaler.transform(x_test) # x_test_scale은 numpy ndarray \n",
    "\n",
    "\n",
    "# Array-->Tensor\n",
    "x_train_tensor = torch.FloatTensor(x_train_scale)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values) #y_train은 판다스 Series이므로 values를 사용해서 numpy ndarray로 가져오기\n",
    "\n",
    "x_test_tensor = torch.FloatTensor(x_test_scale)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -Batchfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([354, 13])\n",
      "torch.Size([354])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#학습 데이터 배치화 시키기 \n",
    "train_data = data_utils.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "dataloader = data_utils.DataLoader(train_data, batch_size=354, shuffle=True , drop_last=True)\n",
    "\n",
    "\n",
    "#배치화된 데이터 확인\n",
    "for batch_idx, datas in enumerate(dataloader):\n",
    "    print(batch_idx)\n",
    "    print(datas[0].shape)  # x_train \n",
    "    print(datas[1].shape) # y_train\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "class MLP_model(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size ,output_size):\n",
    "            super(MLP_model, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.output_size = output_size\n",
    "            self.hidden2 = 16\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden2)\n",
    "            self.fc3 = torch.nn.Linear(self.hidden2, self.output_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sig = torch.nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            fc1 = self.fc1(x)\n",
    "            ac1 = self.relu(fc1)\n",
    "            fc2 = self.fc2(ac1)\n",
    "            ac2 = self.relu(fc2)\n",
    "            fc3 = self.fc3(ac2)\n",
    "            \n",
    "            return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter 정의\n",
    "input_dim = 13\n",
    "output_dim = 1\n",
    "hidden_dim = 32\n",
    "learning_rate = 0.001\n",
    "n_epochs = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 생성\n",
    "model = MLP_model(input_size = input_dim, hidden_size = hidden_dim, output_size = output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수 생성\n",
    "criterion = torch.nn.MSELoss()\n",
    "#Optimizer 생성\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, Loss_train:587.08, Loss_test:600.75\n",
      "epoch:1, Loss_train:583.46, Loss_test:596.84\n",
      "epoch:2, Loss_train:579.65, Loss_test:592.66\n",
      "epoch:3, Loss_train:575.60, Loss_test:588.18\n",
      "epoch:4, Loss_train:571.26, Loss_test:583.34\n",
      "epoch:5, Loss_train:566.57, Loss_test:578.05\n",
      "epoch:6, Loss_train:561.45, Loss_test:572.20\n",
      "epoch:7, Loss_train:555.80, Loss_test:565.71\n",
      "epoch:8, Loss_train:549.52, Loss_test:558.41\n",
      "epoch:9, Loss_train:542.48, Loss_test:550.16\n",
      "epoch:10, Loss_train:534.53, Loss_test:540.75\n",
      "epoch:11, Loss_train:525.49, Loss_test:529.94\n",
      "epoch:12, Loss_train:515.12, Loss_test:517.45\n",
      "epoch:13, Loss_train:503.15, Loss_test:502.94\n",
      "epoch:14, Loss_train:489.27, Loss_test:486.00\n",
      "epoch:15, Loss_train:473.08, Loss_test:466.15\n",
      "epoch:16, Loss_train:454.11, Loss_test:442.85\n",
      "epoch:17, Loss_train:431.86, Loss_test:415.63\n",
      "epoch:18, Loss_train:405.87, Loss_test:384.12\n",
      "epoch:19, Loss_train:375.78, Loss_test:348.30\n",
      "epoch:20, Loss_train:341.49, Loss_test:308.67\n",
      "epoch:21, Loss_train:303.37, Loss_test:266.56\n",
      "epoch:22, Loss_train:262.58, Loss_test:224.36\n",
      "epoch:23, Loss_train:221.25, Loss_test:185.21\n",
      "epoch:24, Loss_train:182.32, Loss_test:152.11\n",
      "epoch:25, Loss_train:148.70, Loss_test:126.75\n",
      "epoch:26, Loss_train:122.24, Loss_test:108.74\n",
      "epoch:27, Loss_train:102.87, Loss_test:96.14\n",
      "epoch:28, Loss_train:89.08, Loss_test:86.81\n",
      "epoch:29, Loss_train:78.93, Loss_test:79.26\n",
      "epoch:30, Loss_train:70.99, Loss_test:72.74\n",
      "epoch:31, Loss_train:64.38, Loss_test:66.98\n",
      "epoch:32, Loss_train:58.72, Loss_test:61.87\n",
      "epoch:33, Loss_train:53.84, Loss_test:57.37\n",
      "epoch:34, Loss_train:49.65, Loss_test:53.44\n",
      "epoch:35, Loss_train:46.06, Loss_test:50.00\n",
      "epoch:36, Loss_train:43.00, Loss_test:46.98\n",
      "epoch:37, Loss_train:40.37, Loss_test:44.35\n",
      "epoch:38, Loss_train:38.13, Loss_test:42.03\n",
      "epoch:39, Loss_train:36.21, Loss_test:39.97\n",
      "epoch:40, Loss_train:34.56, Loss_test:38.14\n",
      "epoch:41, Loss_train:33.14, Loss_test:36.51\n",
      "epoch:42, Loss_train:31.91, Loss_test:35.04\n",
      "epoch:43, Loss_train:30.82, Loss_test:33.70\n",
      "epoch:44, Loss_train:29.86, Loss_test:32.50\n",
      "epoch:45, Loss_train:29.00, Loss_test:31.41\n",
      "epoch:46, Loss_train:28.25, Loss_test:30.44\n",
      "epoch:47, Loss_train:27.59, Loss_test:29.56\n",
      "epoch:48, Loss_train:27.00, Loss_test:28.76\n",
      "epoch:49, Loss_train:26.46, Loss_test:28.03\n",
      "epoch:50, Loss_train:25.98, Loss_test:27.36\n",
      "epoch:51, Loss_train:25.54, Loss_test:26.75\n",
      "epoch:52, Loss_train:25.14, Loss_test:26.17\n",
      "epoch:53, Loss_train:24.77, Loss_test:25.64\n",
      "epoch:54, Loss_train:24.43, Loss_test:25.14\n",
      "epoch:55, Loss_train:24.10, Loss_test:24.68\n",
      "epoch:56, Loss_train:23.80, Loss_test:24.25\n",
      "epoch:57, Loss_train:23.51, Loss_test:23.85\n",
      "epoch:58, Loss_train:23.24, Loss_test:23.47\n",
      "epoch:59, Loss_train:22.98, Loss_test:23.11\n",
      "epoch:60, Loss_train:22.73, Loss_test:22.78\n",
      "epoch:61, Loss_train:22.50, Loss_test:22.48\n",
      "epoch:62, Loss_train:22.28, Loss_test:22.19\n",
      "epoch:63, Loss_train:22.07, Loss_test:21.91\n",
      "epoch:64, Loss_train:21.87, Loss_test:21.66\n",
      "epoch:65, Loss_train:21.67, Loss_test:21.41\n",
      "epoch:66, Loss_train:21.49, Loss_test:21.18\n",
      "epoch:67, Loss_train:21.31, Loss_test:20.96\n",
      "epoch:68, Loss_train:21.14, Loss_test:20.75\n",
      "epoch:69, Loss_train:20.98, Loss_test:20.54\n",
      "epoch:70, Loss_train:20.82, Loss_test:20.35\n",
      "epoch:71, Loss_train:20.66, Loss_test:20.17\n",
      "epoch:72, Loss_train:20.51, Loss_test:19.99\n",
      "epoch:73, Loss_train:20.37, Loss_test:19.83\n",
      "epoch:74, Loss_train:20.23, Loss_test:19.67\n",
      "epoch:75, Loss_train:20.09, Loss_test:19.52\n",
      "epoch:76, Loss_train:19.96, Loss_test:19.38\n",
      "epoch:77, Loss_train:19.84, Loss_test:19.24\n",
      "epoch:78, Loss_train:19.72, Loss_test:19.11\n",
      "epoch:79, Loss_train:19.60, Loss_test:18.98\n",
      "epoch:80, Loss_train:19.48, Loss_test:18.85\n",
      "epoch:81, Loss_train:19.37, Loss_test:18.73\n",
      "epoch:82, Loss_train:19.26, Loss_test:18.61\n",
      "epoch:83, Loss_train:19.15, Loss_test:18.50\n",
      "epoch:84, Loss_train:19.05, Loss_test:18.39\n",
      "epoch:85, Loss_train:18.95, Loss_test:18.28\n",
      "epoch:86, Loss_train:18.85, Loss_test:18.18\n",
      "epoch:87, Loss_train:18.75, Loss_test:18.08\n",
      "epoch:88, Loss_train:18.65, Loss_test:17.98\n",
      "epoch:89, Loss_train:18.56, Loss_test:17.88\n",
      "epoch:90, Loss_train:18.47, Loss_test:17.79\n",
      "epoch:91, Loss_train:18.38, Loss_test:17.70\n",
      "epoch:92, Loss_train:18.29, Loss_test:17.61\n",
      "epoch:93, Loss_train:18.20, Loss_test:17.53\n",
      "epoch:94, Loss_train:18.11, Loss_test:17.44\n",
      "epoch:95, Loss_train:18.03, Loss_test:17.36\n",
      "epoch:96, Loss_train:17.95, Loss_test:17.29\n",
      "epoch:97, Loss_train:17.87, Loss_test:17.21\n",
      "epoch:98, Loss_train:17.79, Loss_test:17.14\n",
      "epoch:99, Loss_train:17.71, Loss_test:17.06\n",
      "epoch:100, Loss_train:17.64, Loss_test:16.99\n",
      "epoch:101, Loss_train:17.56, Loss_test:16.92\n",
      "epoch:102, Loss_train:17.49, Loss_test:16.85\n",
      "epoch:103, Loss_train:17.41, Loss_test:16.78\n",
      "epoch:104, Loss_train:17.34, Loss_test:16.71\n",
      "epoch:105, Loss_train:17.27, Loss_test:16.64\n",
      "epoch:106, Loss_train:17.20, Loss_test:16.58\n",
      "epoch:107, Loss_train:17.14, Loss_test:16.52\n",
      "epoch:108, Loss_train:17.07, Loss_test:16.45\n",
      "epoch:109, Loss_train:17.01, Loss_test:16.39\n",
      "epoch:110, Loss_train:16.94, Loss_test:16.34\n",
      "epoch:111, Loss_train:16.88, Loss_test:16.28\n",
      "epoch:112, Loss_train:16.82, Loss_test:16.22\n",
      "epoch:113, Loss_train:16.76, Loss_test:16.17\n",
      "epoch:114, Loss_train:16.70, Loss_test:16.12\n",
      "epoch:115, Loss_train:16.64, Loss_test:16.07\n",
      "epoch:116, Loss_train:16.58, Loss_test:16.02\n",
      "epoch:117, Loss_train:16.53, Loss_test:15.97\n",
      "epoch:118, Loss_train:16.47, Loss_test:15.92\n",
      "epoch:119, Loss_train:16.42, Loss_test:15.88\n",
      "epoch:120, Loss_train:16.36, Loss_test:15.83\n",
      "epoch:121, Loss_train:16.31, Loss_test:15.79\n",
      "epoch:122, Loss_train:16.25, Loss_test:15.74\n",
      "epoch:123, Loss_train:16.20, Loss_test:15.70\n",
      "epoch:124, Loss_train:16.15, Loss_test:15.66\n",
      "epoch:125, Loss_train:16.10, Loss_test:15.62\n",
      "epoch:126, Loss_train:16.05, Loss_test:15.58\n",
      "epoch:127, Loss_train:16.00, Loss_test:15.54\n",
      "epoch:128, Loss_train:15.95, Loss_test:15.50\n",
      "epoch:129, Loss_train:15.90, Loss_test:15.46\n",
      "epoch:130, Loss_train:15.86, Loss_test:15.42\n",
      "epoch:131, Loss_train:15.81, Loss_test:15.39\n",
      "epoch:132, Loss_train:15.76, Loss_test:15.35\n",
      "epoch:133, Loss_train:15.72, Loss_test:15.31\n",
      "epoch:134, Loss_train:15.67, Loss_test:15.28\n",
      "epoch:135, Loss_train:15.63, Loss_test:15.24\n",
      "epoch:136, Loss_train:15.58, Loss_test:15.21\n",
      "epoch:137, Loss_train:15.54, Loss_test:15.18\n",
      "epoch:138, Loss_train:15.50, Loss_test:15.14\n",
      "epoch:139, Loss_train:15.45, Loss_test:15.11\n",
      "epoch:140, Loss_train:15.41, Loss_test:15.08\n",
      "epoch:141, Loss_train:15.37, Loss_test:15.05\n",
      "epoch:142, Loss_train:15.33, Loss_test:15.02\n",
      "epoch:143, Loss_train:15.29, Loss_test:14.99\n",
      "epoch:144, Loss_train:15.25, Loss_test:14.96\n",
      "epoch:145, Loss_train:15.21, Loss_test:14.93\n",
      "epoch:146, Loss_train:15.17, Loss_test:14.90\n",
      "epoch:147, Loss_train:15.14, Loss_test:14.87\n",
      "epoch:148, Loss_train:15.10, Loss_test:14.84\n",
      "epoch:149, Loss_train:15.06, Loss_test:14.82\n",
      "epoch:150, Loss_train:15.02, Loss_test:14.79\n",
      "epoch:151, Loss_train:14.99, Loss_test:14.76\n",
      "epoch:152, Loss_train:14.95, Loss_test:14.74\n",
      "epoch:153, Loss_train:14.92, Loss_test:14.71\n",
      "epoch:154, Loss_train:14.88, Loss_test:14.68\n",
      "epoch:155, Loss_train:14.85, Loss_test:14.66\n",
      "epoch:156, Loss_train:14.81, Loss_test:14.63\n",
      "epoch:157, Loss_train:14.78, Loss_test:14.61\n",
      "epoch:158, Loss_train:14.74, Loss_test:14.58\n",
      "epoch:159, Loss_train:14.71, Loss_test:14.56\n",
      "epoch:160, Loss_train:14.68, Loss_test:14.53\n",
      "epoch:161, Loss_train:14.65, Loss_test:14.51\n",
      "epoch:162, Loss_train:14.61, Loss_test:14.49\n",
      "epoch:163, Loss_train:14.58, Loss_test:14.46\n",
      "epoch:164, Loss_train:14.55, Loss_test:14.44\n",
      "epoch:165, Loss_train:14.52, Loss_test:14.42\n",
      "epoch:166, Loss_train:14.48, Loss_test:14.40\n",
      "epoch:167, Loss_train:14.45, Loss_test:14.37\n",
      "epoch:168, Loss_train:14.42, Loss_test:14.35\n",
      "epoch:169, Loss_train:14.39, Loss_test:14.33\n",
      "epoch:170, Loss_train:14.36, Loss_test:14.31\n",
      "epoch:171, Loss_train:14.33, Loss_test:14.28\n",
      "epoch:172, Loss_train:14.30, Loss_test:14.26\n",
      "epoch:173, Loss_train:14.27, Loss_test:14.24\n",
      "epoch:174, Loss_train:14.25, Loss_test:14.22\n",
      "epoch:175, Loss_train:14.22, Loss_test:14.20\n",
      "epoch:176, Loss_train:14.19, Loss_test:14.18\n",
      "epoch:177, Loss_train:14.16, Loss_test:14.16\n",
      "epoch:178, Loss_train:14.13, Loss_test:14.14\n",
      "epoch:179, Loss_train:14.11, Loss_test:14.13\n",
      "epoch:180, Loss_train:14.08, Loss_test:14.11\n",
      "epoch:181, Loss_train:14.05, Loss_test:14.09\n",
      "epoch:182, Loss_train:14.03, Loss_test:14.07\n",
      "epoch:183, Loss_train:14.00, Loss_test:14.05\n",
      "epoch:184, Loss_train:13.98, Loss_test:14.04\n",
      "epoch:185, Loss_train:13.95, Loss_test:14.02\n",
      "epoch:186, Loss_train:13.93, Loss_test:14.00\n",
      "epoch:187, Loss_train:13.90, Loss_test:13.98\n",
      "epoch:188, Loss_train:13.88, Loss_test:13.96\n",
      "epoch:189, Loss_train:13.85, Loss_test:13.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:190, Loss_train:13.83, Loss_test:13.93\n",
      "epoch:191, Loss_train:13.81, Loss_test:13.91\n",
      "epoch:192, Loss_train:13.78, Loss_test:13.90\n",
      "epoch:193, Loss_train:13.76, Loss_test:13.88\n",
      "epoch:194, Loss_train:13.74, Loss_test:13.86\n",
      "epoch:195, Loss_train:13.71, Loss_test:13.84\n",
      "epoch:196, Loss_train:13.69, Loss_test:13.83\n",
      "epoch:197, Loss_train:13.67, Loss_test:13.81\n",
      "epoch:198, Loss_train:13.65, Loss_test:13.80\n",
      "epoch:199, Loss_train:13.63, Loss_test:13.78\n",
      "epoch:200, Loss_train:13.60, Loss_test:13.77\n",
      "epoch:201, Loss_train:13.58, Loss_test:13.75\n",
      "epoch:202, Loss_train:13.56, Loss_test:13.74\n",
      "epoch:203, Loss_train:13.54, Loss_test:13.72\n",
      "epoch:204, Loss_train:13.52, Loss_test:13.71\n",
      "epoch:205, Loss_train:13.50, Loss_test:13.69\n",
      "epoch:206, Loss_train:13.48, Loss_test:13.68\n",
      "epoch:207, Loss_train:13.46, Loss_test:13.66\n",
      "epoch:208, Loss_train:13.44, Loss_test:13.65\n",
      "epoch:209, Loss_train:13.42, Loss_test:13.63\n",
      "epoch:210, Loss_train:13.40, Loss_test:13.62\n",
      "epoch:211, Loss_train:13.38, Loss_test:13.60\n",
      "epoch:212, Loss_train:13.36, Loss_test:13.59\n",
      "epoch:213, Loss_train:13.34, Loss_test:13.58\n",
      "epoch:214, Loss_train:13.32, Loss_test:13.56\n",
      "epoch:215, Loss_train:13.30, Loss_test:13.55\n",
      "epoch:216, Loss_train:13.28, Loss_test:13.53\n",
      "epoch:217, Loss_train:13.26, Loss_test:13.52\n",
      "epoch:218, Loss_train:13.24, Loss_test:13.51\n",
      "epoch:219, Loss_train:13.22, Loss_test:13.49\n",
      "epoch:220, Loss_train:13.20, Loss_test:13.48\n",
      "epoch:221, Loss_train:13.19, Loss_test:13.47\n",
      "epoch:222, Loss_train:13.17, Loss_test:13.45\n",
      "epoch:223, Loss_train:13.15, Loss_test:13.44\n",
      "epoch:224, Loss_train:13.13, Loss_test:13.42\n",
      "epoch:225, Loss_train:13.11, Loss_test:13.41\n",
      "epoch:226, Loss_train:13.10, Loss_test:13.40\n",
      "epoch:227, Loss_train:13.08, Loss_test:13.39\n",
      "epoch:228, Loss_train:13.06, Loss_test:13.37\n",
      "epoch:229, Loss_train:13.05, Loss_test:13.36\n",
      "epoch:230, Loss_train:13.03, Loss_test:13.35\n",
      "epoch:231, Loss_train:13.01, Loss_test:13.34\n",
      "epoch:232, Loss_train:13.00, Loss_test:13.32\n",
      "epoch:233, Loss_train:12.98, Loss_test:13.31\n",
      "epoch:234, Loss_train:12.96, Loss_test:13.30\n",
      "epoch:235, Loss_train:12.95, Loss_test:13.29\n",
      "epoch:236, Loss_train:12.93, Loss_test:13.28\n",
      "epoch:237, Loss_train:12.92, Loss_test:13.27\n",
      "epoch:238, Loss_train:12.90, Loss_test:13.25\n",
      "epoch:239, Loss_train:12.88, Loss_test:13.24\n",
      "epoch:240, Loss_train:12.87, Loss_test:13.23\n",
      "epoch:241, Loss_train:12.85, Loss_test:13.22\n",
      "epoch:242, Loss_train:12.84, Loss_test:13.21\n",
      "epoch:243, Loss_train:12.82, Loss_test:13.20\n",
      "epoch:244, Loss_train:12.81, Loss_test:13.19\n",
      "epoch:245, Loss_train:12.79, Loss_test:13.18\n",
      "epoch:246, Loss_train:12.78, Loss_test:13.17\n",
      "epoch:247, Loss_train:12.76, Loss_test:13.15\n",
      "epoch:248, Loss_train:12.75, Loss_test:13.14\n",
      "epoch:249, Loss_train:12.73, Loss_test:13.13\n",
      "epoch:250, Loss_train:12.72, Loss_test:13.12\n",
      "epoch:251, Loss_train:12.71, Loss_test:13.11\n",
      "epoch:252, Loss_train:12.69, Loss_test:13.10\n",
      "epoch:253, Loss_train:12.68, Loss_test:13.09\n",
      "epoch:254, Loss_train:12.66, Loss_test:13.08\n",
      "epoch:255, Loss_train:12.65, Loss_test:13.07\n",
      "epoch:256, Loss_train:12.63, Loss_test:13.06\n",
      "epoch:257, Loss_train:12.62, Loss_test:13.05\n",
      "epoch:258, Loss_train:12.61, Loss_test:13.03\n",
      "epoch:259, Loss_train:12.59, Loss_test:13.02\n",
      "epoch:260, Loss_train:12.58, Loss_test:13.01\n",
      "epoch:261, Loss_train:12.56, Loss_test:13.00\n",
      "epoch:262, Loss_train:12.55, Loss_test:12.99\n",
      "epoch:263, Loss_train:12.54, Loss_test:12.98\n",
      "epoch:264, Loss_train:12.52, Loss_test:12.97\n",
      "epoch:265, Loss_train:12.51, Loss_test:12.96\n",
      "epoch:266, Loss_train:12.50, Loss_test:12.95\n",
      "epoch:267, Loss_train:12.48, Loss_test:12.94\n",
      "epoch:268, Loss_train:12.47, Loss_test:12.94\n",
      "epoch:269, Loss_train:12.45, Loss_test:12.92\n",
      "epoch:270, Loss_train:12.44, Loss_test:12.92\n",
      "epoch:271, Loss_train:12.42, Loss_test:12.90\n",
      "epoch:272, Loss_train:12.41, Loss_test:12.89\n",
      "epoch:273, Loss_train:12.39, Loss_test:12.88\n",
      "epoch:274, Loss_train:12.38, Loss_test:12.87\n",
      "epoch:275, Loss_train:12.37, Loss_test:12.86\n",
      "epoch:276, Loss_train:12.35, Loss_test:12.85\n",
      "epoch:277, Loss_train:12.34, Loss_test:12.85\n",
      "epoch:278, Loss_train:12.32, Loss_test:12.83\n",
      "epoch:279, Loss_train:12.31, Loss_test:12.83\n",
      "epoch:280, Loss_train:12.30, Loss_test:12.81\n",
      "epoch:281, Loss_train:12.28, Loss_test:12.81\n",
      "epoch:282, Loss_train:12.27, Loss_test:12.79\n",
      "epoch:283, Loss_train:12.25, Loss_test:12.79\n",
      "epoch:284, Loss_train:12.24, Loss_test:12.78\n",
      "epoch:285, Loss_train:12.22, Loss_test:12.77\n",
      "epoch:286, Loss_train:12.21, Loss_test:12.76\n",
      "epoch:287, Loss_train:12.20, Loss_test:12.75\n",
      "epoch:288, Loss_train:12.18, Loss_test:12.74\n",
      "epoch:289, Loss_train:12.17, Loss_test:12.73\n",
      "epoch:290, Loss_train:12.16, Loss_test:12.72\n",
      "epoch:291, Loss_train:12.14, Loss_test:12.71\n",
      "epoch:292, Loss_train:12.13, Loss_test:12.70\n",
      "epoch:293, Loss_train:12.11, Loss_test:12.69\n",
      "epoch:294, Loss_train:12.10, Loss_test:12.68\n",
      "epoch:295, Loss_train:12.09, Loss_test:12.67\n",
      "epoch:296, Loss_train:12.07, Loss_test:12.66\n",
      "epoch:297, Loss_train:12.06, Loss_test:12.65\n",
      "epoch:298, Loss_train:12.05, Loss_test:12.64\n",
      "epoch:299, Loss_train:12.03, Loss_test:12.63\n",
      "epoch:300, Loss_train:12.02, Loss_test:12.62\n",
      "epoch:301, Loss_train:12.01, Loss_test:12.61\n",
      "epoch:302, Loss_train:12.00, Loss_test:12.60\n",
      "epoch:303, Loss_train:11.98, Loss_test:12.59\n",
      "epoch:304, Loss_train:11.97, Loss_test:12.58\n",
      "epoch:305, Loss_train:11.96, Loss_test:12.58\n",
      "epoch:306, Loss_train:11.95, Loss_test:12.57\n",
      "epoch:307, Loss_train:11.93, Loss_test:12.56\n",
      "epoch:308, Loss_train:11.92, Loss_test:12.54\n",
      "epoch:309, Loss_train:11.91, Loss_test:12.54\n",
      "epoch:310, Loss_train:11.90, Loss_test:12.53\n",
      "epoch:311, Loss_train:11.88, Loss_test:12.52\n",
      "epoch:312, Loss_train:11.87, Loss_test:12.50\n",
      "epoch:313, Loss_train:11.86, Loss_test:12.50\n",
      "epoch:314, Loss_train:11.85, Loss_test:12.49\n",
      "epoch:315, Loss_train:11.84, Loss_test:12.48\n",
      "epoch:316, Loss_train:11.82, Loss_test:12.46\n",
      "epoch:317, Loss_train:11.81, Loss_test:12.46\n",
      "epoch:318, Loss_train:11.80, Loss_test:12.45\n",
      "epoch:319, Loss_train:11.79, Loss_test:12.44\n",
      "epoch:320, Loss_train:11.78, Loss_test:12.43\n",
      "epoch:321, Loss_train:11.76, Loss_test:12.42\n",
      "epoch:322, Loss_train:11.75, Loss_test:12.41\n",
      "epoch:323, Loss_train:11.74, Loss_test:12.40\n",
      "epoch:324, Loss_train:11.73, Loss_test:12.39\n",
      "epoch:325, Loss_train:11.72, Loss_test:12.38\n",
      "epoch:326, Loss_train:11.71, Loss_test:12.37\n",
      "epoch:327, Loss_train:11.70, Loss_test:12.36\n",
      "epoch:328, Loss_train:11.68, Loss_test:12.35\n",
      "epoch:329, Loss_train:11.67, Loss_test:12.34\n",
      "epoch:330, Loss_train:11.66, Loss_test:12.34\n",
      "epoch:331, Loss_train:11.65, Loss_test:12.33\n",
      "epoch:332, Loss_train:11.64, Loss_test:12.32\n",
      "epoch:333, Loss_train:11.63, Loss_test:12.31\n",
      "epoch:334, Loss_train:11.62, Loss_test:12.30\n",
      "epoch:335, Loss_train:11.61, Loss_test:12.30\n",
      "epoch:336, Loss_train:11.59, Loss_test:12.29\n",
      "epoch:337, Loss_train:11.58, Loss_test:12.28\n",
      "epoch:338, Loss_train:11.57, Loss_test:12.27\n",
      "epoch:339, Loss_train:11.56, Loss_test:12.26\n",
      "epoch:340, Loss_train:11.55, Loss_test:12.26\n",
      "epoch:341, Loss_train:11.54, Loss_test:12.25\n",
      "epoch:342, Loss_train:11.53, Loss_test:12.24\n",
      "epoch:343, Loss_train:11.52, Loss_test:12.23\n",
      "epoch:344, Loss_train:11.51, Loss_test:12.22\n",
      "epoch:345, Loss_train:11.50, Loss_test:12.22\n",
      "epoch:346, Loss_train:11.49, Loss_test:12.21\n",
      "epoch:347, Loss_train:11.47, Loss_test:12.20\n",
      "epoch:348, Loss_train:11.46, Loss_test:12.19\n",
      "epoch:349, Loss_train:11.45, Loss_test:12.18\n",
      "epoch:350, Loss_train:11.44, Loss_test:12.18\n",
      "epoch:351, Loss_train:11.43, Loss_test:12.17\n",
      "epoch:352, Loss_train:11.42, Loss_test:12.16\n",
      "epoch:353, Loss_train:11.41, Loss_test:12.15\n",
      "epoch:354, Loss_train:11.40, Loss_test:12.14\n",
      "epoch:355, Loss_train:11.39, Loss_test:12.14\n",
      "epoch:356, Loss_train:11.38, Loss_test:12.13\n",
      "epoch:357, Loss_train:11.37, Loss_test:12.12\n",
      "epoch:358, Loss_train:11.36, Loss_test:12.11\n",
      "epoch:359, Loss_train:11.35, Loss_test:12.11\n",
      "epoch:360, Loss_train:11.34, Loss_test:12.10\n",
      "epoch:361, Loss_train:11.33, Loss_test:12.09\n",
      "epoch:362, Loss_train:11.32, Loss_test:12.08\n",
      "epoch:363, Loss_train:11.31, Loss_test:12.08\n",
      "epoch:364, Loss_train:11.30, Loss_test:12.07\n",
      "epoch:365, Loss_train:11.29, Loss_test:12.06\n",
      "epoch:366, Loss_train:11.28, Loss_test:12.05\n",
      "epoch:367, Loss_train:11.27, Loss_test:12.05\n",
      "epoch:368, Loss_train:11.26, Loss_test:12.04\n",
      "epoch:369, Loss_train:11.25, Loss_test:12.03\n",
      "epoch:370, Loss_train:11.24, Loss_test:12.03\n",
      "epoch:371, Loss_train:11.23, Loss_test:12.02\n",
      "epoch:372, Loss_train:11.22, Loss_test:12.01\n",
      "epoch:373, Loss_train:11.21, Loss_test:12.01\n",
      "epoch:374, Loss_train:11.20, Loss_test:12.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:375, Loss_train:11.19, Loss_test:11.99\n",
      "epoch:376, Loss_train:11.18, Loss_test:11.99\n",
      "epoch:377, Loss_train:11.17, Loss_test:11.98\n",
      "epoch:378, Loss_train:11.16, Loss_test:11.97\n",
      "epoch:379, Loss_train:11.15, Loss_test:11.97\n",
      "epoch:380, Loss_train:11.14, Loss_test:11.96\n",
      "epoch:381, Loss_train:11.13, Loss_test:11.96\n",
      "epoch:382, Loss_train:11.12, Loss_test:11.95\n",
      "epoch:383, Loss_train:11.11, Loss_test:11.95\n",
      "epoch:384, Loss_train:11.10, Loss_test:11.94\n",
      "epoch:385, Loss_train:11.10, Loss_test:11.94\n",
      "epoch:386, Loss_train:11.09, Loss_test:11.93\n",
      "epoch:387, Loss_train:11.08, Loss_test:11.93\n",
      "epoch:388, Loss_train:11.07, Loss_test:11.92\n",
      "epoch:389, Loss_train:11.06, Loss_test:11.92\n",
      "epoch:390, Loss_train:11.05, Loss_test:11.91\n",
      "epoch:391, Loss_train:11.04, Loss_test:11.91\n",
      "epoch:392, Loss_train:11.03, Loss_test:11.90\n",
      "epoch:393, Loss_train:11.02, Loss_test:11.90\n",
      "epoch:394, Loss_train:11.01, Loss_test:11.89\n",
      "epoch:395, Loss_train:11.00, Loss_test:11.89\n",
      "epoch:396, Loss_train:10.99, Loss_test:11.88\n",
      "epoch:397, Loss_train:10.99, Loss_test:11.88\n",
      "epoch:398, Loss_train:10.98, Loss_test:11.87\n",
      "epoch:399, Loss_train:10.97, Loss_test:11.87\n",
      "epoch:400, Loss_train:10.96, Loss_test:11.86\n",
      "epoch:401, Loss_train:10.95, Loss_test:11.86\n",
      "epoch:402, Loss_train:10.94, Loss_test:11.85\n",
      "epoch:403, Loss_train:10.93, Loss_test:11.85\n",
      "epoch:404, Loss_train:10.92, Loss_test:11.85\n",
      "epoch:405, Loss_train:10.91, Loss_test:11.84\n",
      "epoch:406, Loss_train:10.91, Loss_test:11.84\n",
      "epoch:407, Loss_train:10.90, Loss_test:11.83\n",
      "epoch:408, Loss_train:10.89, Loss_test:11.83\n",
      "epoch:409, Loss_train:10.88, Loss_test:11.82\n",
      "epoch:410, Loss_train:10.87, Loss_test:11.82\n",
      "epoch:411, Loss_train:10.86, Loss_test:11.81\n",
      "epoch:412, Loss_train:10.85, Loss_test:11.81\n",
      "epoch:413, Loss_train:10.85, Loss_test:11.81\n",
      "epoch:414, Loss_train:10.84, Loss_test:11.80\n",
      "epoch:415, Loss_train:10.83, Loss_test:11.80\n",
      "epoch:416, Loss_train:10.82, Loss_test:11.79\n",
      "epoch:417, Loss_train:10.81, Loss_test:11.79\n",
      "epoch:418, Loss_train:10.80, Loss_test:11.78\n",
      "epoch:419, Loss_train:10.79, Loss_test:11.78\n",
      "epoch:420, Loss_train:10.79, Loss_test:11.78\n",
      "epoch:421, Loss_train:10.78, Loss_test:11.77\n",
      "epoch:422, Loss_train:10.77, Loss_test:11.77\n",
      "epoch:423, Loss_train:10.76, Loss_test:11.76\n",
      "epoch:424, Loss_train:10.75, Loss_test:11.76\n",
      "epoch:425, Loss_train:10.74, Loss_test:11.76\n",
      "epoch:426, Loss_train:10.74, Loss_test:11.75\n",
      "epoch:427, Loss_train:10.73, Loss_test:11.75\n",
      "epoch:428, Loss_train:10.72, Loss_test:11.75\n",
      "epoch:429, Loss_train:10.71, Loss_test:11.74\n",
      "epoch:430, Loss_train:10.71, Loss_test:11.74\n",
      "epoch:431, Loss_train:10.70, Loss_test:11.73\n",
      "epoch:432, Loss_train:10.69, Loss_test:11.73\n",
      "epoch:433, Loss_train:10.68, Loss_test:11.73\n",
      "epoch:434, Loss_train:10.67, Loss_test:11.72\n",
      "epoch:435, Loss_train:10.67, Loss_test:11.72\n",
      "epoch:436, Loss_train:10.66, Loss_test:11.72\n",
      "epoch:437, Loss_train:10.65, Loss_test:11.71\n",
      "epoch:438, Loss_train:10.64, Loss_test:11.71\n",
      "epoch:439, Loss_train:10.64, Loss_test:11.70\n",
      "epoch:440, Loss_train:10.63, Loss_test:11.70\n",
      "epoch:441, Loss_train:10.62, Loss_test:11.70\n",
      "epoch:442, Loss_train:10.61, Loss_test:11.69\n",
      "epoch:443, Loss_train:10.60, Loss_test:11.69\n",
      "epoch:444, Loss_train:10.60, Loss_test:11.69\n",
      "epoch:445, Loss_train:10.59, Loss_test:11.68\n",
      "epoch:446, Loss_train:10.58, Loss_test:11.68\n",
      "epoch:447, Loss_train:10.57, Loss_test:11.68\n",
      "epoch:448, Loss_train:10.56, Loss_test:11.67\n",
      "epoch:449, Loss_train:10.55, Loss_test:11.67\n",
      "epoch:450, Loss_train:10.55, Loss_test:11.67\n",
      "epoch:451, Loss_train:10.54, Loss_test:11.66\n",
      "epoch:452, Loss_train:10.53, Loss_test:11.66\n",
      "epoch:453, Loss_train:10.52, Loss_test:11.65\n",
      "epoch:454, Loss_train:10.51, Loss_test:11.65\n",
      "epoch:455, Loss_train:10.51, Loss_test:11.65\n",
      "epoch:456, Loss_train:10.50, Loss_test:11.64\n",
      "epoch:457, Loss_train:10.49, Loss_test:11.64\n",
      "epoch:458, Loss_train:10.48, Loss_test:11.64\n",
      "epoch:459, Loss_train:10.47, Loss_test:11.63\n",
      "epoch:460, Loss_train:10.46, Loss_test:11.63\n",
      "epoch:461, Loss_train:10.45, Loss_test:11.63\n",
      "epoch:462, Loss_train:10.45, Loss_test:11.62\n",
      "epoch:463, Loss_train:10.44, Loss_test:11.62\n",
      "epoch:464, Loss_train:10.43, Loss_test:11.62\n",
      "epoch:465, Loss_train:10.42, Loss_test:11.61\n",
      "epoch:466, Loss_train:10.41, Loss_test:11.61\n",
      "epoch:467, Loss_train:10.41, Loss_test:11.60\n",
      "epoch:468, Loss_train:10.40, Loss_test:11.60\n",
      "epoch:469, Loss_train:10.39, Loss_test:11.60\n",
      "epoch:470, Loss_train:10.38, Loss_test:11.60\n",
      "epoch:471, Loss_train:10.37, Loss_test:11.59\n",
      "epoch:472, Loss_train:10.36, Loss_test:11.59\n",
      "epoch:473, Loss_train:10.36, Loss_test:11.59\n",
      "epoch:474, Loss_train:10.35, Loss_test:11.58\n",
      "epoch:475, Loss_train:10.34, Loss_test:11.58\n",
      "epoch:476, Loss_train:10.33, Loss_test:11.58\n",
      "epoch:477, Loss_train:10.32, Loss_test:11.57\n",
      "epoch:478, Loss_train:10.32, Loss_test:11.57\n",
      "epoch:479, Loss_train:10.31, Loss_test:11.57\n",
      "epoch:480, Loss_train:10.30, Loss_test:11.56\n",
      "epoch:481, Loss_train:10.29, Loss_test:11.56\n",
      "epoch:482, Loss_train:10.28, Loss_test:11.56\n",
      "epoch:483, Loss_train:10.28, Loss_test:11.56\n",
      "epoch:484, Loss_train:10.27, Loss_test:11.55\n",
      "epoch:485, Loss_train:10.26, Loss_test:11.55\n",
      "epoch:486, Loss_train:10.25, Loss_test:11.55\n",
      "epoch:487, Loss_train:10.25, Loss_test:11.54\n",
      "epoch:488, Loss_train:10.24, Loss_test:11.54\n",
      "epoch:489, Loss_train:10.23, Loss_test:11.54\n",
      "epoch:490, Loss_train:10.22, Loss_test:11.53\n",
      "epoch:491, Loss_train:10.21, Loss_test:11.53\n",
      "epoch:492, Loss_train:10.21, Loss_test:11.53\n",
      "epoch:493, Loss_train:10.20, Loss_test:11.52\n",
      "epoch:494, Loss_train:10.19, Loss_test:11.52\n",
      "epoch:495, Loss_train:10.18, Loss_test:11.52\n",
      "epoch:496, Loss_train:10.18, Loss_test:11.51\n",
      "epoch:497, Loss_train:10.17, Loss_test:11.51\n",
      "epoch:498, Loss_train:10.16, Loss_test:11.51\n",
      "epoch:499, Loss_train:10.15, Loss_test:11.51\n",
      "epoch:500, Loss_train:10.15, Loss_test:11.50\n",
      "epoch:501, Loss_train:10.14, Loss_test:11.50\n",
      "epoch:502, Loss_train:10.13, Loss_test:11.50\n",
      "epoch:503, Loss_train:10.12, Loss_test:11.50\n",
      "epoch:504, Loss_train:10.12, Loss_test:11.49\n",
      "epoch:505, Loss_train:10.11, Loss_test:11.49\n",
      "epoch:506, Loss_train:10.10, Loss_test:11.49\n",
      "epoch:507, Loss_train:10.09, Loss_test:11.49\n",
      "epoch:508, Loss_train:10.09, Loss_test:11.48\n",
      "epoch:509, Loss_train:10.08, Loss_test:11.48\n",
      "epoch:510, Loss_train:10.07, Loss_test:11.48\n",
      "epoch:511, Loss_train:10.06, Loss_test:11.48\n",
      "epoch:512, Loss_train:10.06, Loss_test:11.47\n",
      "epoch:513, Loss_train:10.05, Loss_test:11.47\n",
      "epoch:514, Loss_train:10.04, Loss_test:11.47\n",
      "epoch:515, Loss_train:10.04, Loss_test:11.46\n",
      "epoch:516, Loss_train:10.03, Loss_test:11.46\n",
      "epoch:517, Loss_train:10.02, Loss_test:11.46\n",
      "epoch:518, Loss_train:10.02, Loss_test:11.46\n",
      "epoch:519, Loss_train:10.01, Loss_test:11.45\n",
      "epoch:520, Loss_train:10.00, Loss_test:11.45\n",
      "epoch:521, Loss_train:10.00, Loss_test:11.45\n",
      "epoch:522, Loss_train:9.99, Loss_test:11.44\n",
      "epoch:523, Loss_train:9.98, Loss_test:11.44\n",
      "epoch:524, Loss_train:9.98, Loss_test:11.44\n",
      "epoch:525, Loss_train:9.97, Loss_test:11.43\n",
      "epoch:526, Loss_train:9.97, Loss_test:11.43\n",
      "epoch:527, Loss_train:9.96, Loss_test:11.43\n",
      "epoch:528, Loss_train:9.95, Loss_test:11.42\n",
      "epoch:529, Loss_train:9.95, Loss_test:11.42\n",
      "epoch:530, Loss_train:9.94, Loss_test:11.42\n",
      "epoch:531, Loss_train:9.94, Loss_test:11.42\n",
      "epoch:532, Loss_train:9.93, Loss_test:11.41\n",
      "epoch:533, Loss_train:9.92, Loss_test:11.41\n",
      "epoch:534, Loss_train:9.92, Loss_test:11.41\n",
      "epoch:535, Loss_train:9.91, Loss_test:11.40\n",
      "epoch:536, Loss_train:9.91, Loss_test:11.40\n",
      "epoch:537, Loss_train:9.90, Loss_test:11.40\n",
      "epoch:538, Loss_train:9.89, Loss_test:11.40\n",
      "epoch:539, Loss_train:9.89, Loss_test:11.39\n",
      "epoch:540, Loss_train:9.88, Loss_test:11.39\n",
      "epoch:541, Loss_train:9.88, Loss_test:11.39\n",
      "epoch:542, Loss_train:9.87, Loss_test:11.39\n",
      "epoch:543, Loss_train:9.87, Loss_test:11.39\n",
      "epoch:544, Loss_train:9.86, Loss_test:11.39\n",
      "epoch:545, Loss_train:9.86, Loss_test:11.38\n",
      "epoch:546, Loss_train:9.85, Loss_test:11.38\n",
      "epoch:547, Loss_train:9.84, Loss_test:11.38\n",
      "epoch:548, Loss_train:9.84, Loss_test:11.38\n",
      "epoch:549, Loss_train:9.83, Loss_test:11.37\n",
      "epoch:550, Loss_train:9.83, Loss_test:11.37\n",
      "epoch:551, Loss_train:9.82, Loss_test:11.37\n",
      "epoch:552, Loss_train:9.82, Loss_test:11.37\n",
      "epoch:553, Loss_train:9.81, Loss_test:11.37\n",
      "epoch:554, Loss_train:9.81, Loss_test:11.36\n",
      "epoch:555, Loss_train:9.80, Loss_test:11.36\n",
      "epoch:556, Loss_train:9.80, Loss_test:11.36\n",
      "epoch:557, Loss_train:9.79, Loss_test:11.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:558, Loss_train:9.79, Loss_test:11.35\n",
      "epoch:559, Loss_train:9.78, Loss_test:11.35\n",
      "epoch:560, Loss_train:9.77, Loss_test:11.35\n",
      "epoch:561, Loss_train:9.77, Loss_test:11.35\n",
      "epoch:562, Loss_train:9.76, Loss_test:11.34\n",
      "epoch:563, Loss_train:9.76, Loss_test:11.34\n",
      "epoch:564, Loss_train:9.75, Loss_test:11.34\n",
      "epoch:565, Loss_train:9.75, Loss_test:11.33\n",
      "epoch:566, Loss_train:9.74, Loss_test:11.33\n",
      "epoch:567, Loss_train:9.74, Loss_test:11.33\n",
      "epoch:568, Loss_train:9.73, Loss_test:11.32\n",
      "epoch:569, Loss_train:9.73, Loss_test:11.32\n",
      "epoch:570, Loss_train:9.72, Loss_test:11.32\n",
      "epoch:571, Loss_train:9.72, Loss_test:11.32\n",
      "epoch:572, Loss_train:9.71, Loss_test:11.31\n",
      "epoch:573, Loss_train:9.71, Loss_test:11.31\n",
      "epoch:574, Loss_train:9.70, Loss_test:11.31\n",
      "epoch:575, Loss_train:9.70, Loss_test:11.30\n",
      "epoch:576, Loss_train:9.69, Loss_test:11.30\n",
      "epoch:577, Loss_train:9.69, Loss_test:11.30\n",
      "epoch:578, Loss_train:9.68, Loss_test:11.30\n",
      "epoch:579, Loss_train:9.68, Loss_test:11.29\n",
      "epoch:580, Loss_train:9.67, Loss_test:11.29\n",
      "epoch:581, Loss_train:9.67, Loss_test:11.29\n",
      "epoch:582, Loss_train:9.66, Loss_test:11.28\n",
      "epoch:583, Loss_train:9.66, Loss_test:11.28\n",
      "epoch:584, Loss_train:9.65, Loss_test:11.28\n",
      "epoch:585, Loss_train:9.65, Loss_test:11.27\n",
      "epoch:586, Loss_train:9.64, Loss_test:11.27\n",
      "epoch:587, Loss_train:9.64, Loss_test:11.27\n",
      "epoch:588, Loss_train:9.63, Loss_test:11.26\n",
      "epoch:589, Loss_train:9.63, Loss_test:11.26\n",
      "epoch:590, Loss_train:9.62, Loss_test:11.26\n",
      "epoch:591, Loss_train:9.62, Loss_test:11.25\n",
      "epoch:592, Loss_train:9.61, Loss_test:11.25\n",
      "epoch:593, Loss_train:9.61, Loss_test:11.25\n",
      "epoch:594, Loss_train:9.60, Loss_test:11.25\n",
      "epoch:595, Loss_train:9.60, Loss_test:11.24\n",
      "epoch:596, Loss_train:9.59, Loss_test:11.24\n",
      "epoch:597, Loss_train:9.59, Loss_test:11.24\n",
      "epoch:598, Loss_train:9.58, Loss_test:11.23\n",
      "epoch:599, Loss_train:9.58, Loss_test:11.23\n",
      "epoch:600, Loss_train:9.57, Loss_test:11.23\n",
      "epoch:601, Loss_train:9.57, Loss_test:11.23\n",
      "epoch:602, Loss_train:9.56, Loss_test:11.22\n",
      "epoch:603, Loss_train:9.56, Loss_test:11.22\n",
      "epoch:604, Loss_train:9.55, Loss_test:11.22\n",
      "epoch:605, Loss_train:9.55, Loss_test:11.22\n",
      "epoch:606, Loss_train:9.54, Loss_test:11.21\n",
      "epoch:607, Loss_train:9.54, Loss_test:11.21\n",
      "epoch:608, Loss_train:9.53, Loss_test:11.21\n",
      "epoch:609, Loss_train:9.53, Loss_test:11.21\n",
      "epoch:610, Loss_train:9.52, Loss_test:11.21\n",
      "epoch:611, Loss_train:9.52, Loss_test:11.20\n",
      "epoch:612, Loss_train:9.51, Loss_test:11.20\n",
      "epoch:613, Loss_train:9.51, Loss_test:11.20\n",
      "epoch:614, Loss_train:9.50, Loss_test:11.20\n",
      "epoch:615, Loss_train:9.50, Loss_test:11.19\n",
      "epoch:616, Loss_train:9.49, Loss_test:11.19\n",
      "epoch:617, Loss_train:9.49, Loss_test:11.19\n",
      "epoch:618, Loss_train:9.48, Loss_test:11.19\n",
      "epoch:619, Loss_train:9.48, Loss_test:11.18\n",
      "epoch:620, Loss_train:9.47, Loss_test:11.18\n",
      "epoch:621, Loss_train:9.47, Loss_test:11.18\n",
      "epoch:622, Loss_train:9.46, Loss_test:11.18\n",
      "epoch:623, Loss_train:9.46, Loss_test:11.17\n",
      "epoch:624, Loss_train:9.45, Loss_test:11.17\n",
      "epoch:625, Loss_train:9.45, Loss_test:11.17\n",
      "epoch:626, Loss_train:9.44, Loss_test:11.17\n",
      "epoch:627, Loss_train:9.44, Loss_test:11.17\n",
      "epoch:628, Loss_train:9.44, Loss_test:11.16\n",
      "epoch:629, Loss_train:9.43, Loss_test:11.16\n",
      "epoch:630, Loss_train:9.43, Loss_test:11.16\n",
      "epoch:631, Loss_train:9.42, Loss_test:11.16\n",
      "epoch:632, Loss_train:9.42, Loss_test:11.16\n",
      "epoch:633, Loss_train:9.41, Loss_test:11.15\n",
      "epoch:634, Loss_train:9.41, Loss_test:11.15\n",
      "epoch:635, Loss_train:9.40, Loss_test:11.15\n",
      "epoch:636, Loss_train:9.40, Loss_test:11.15\n",
      "epoch:637, Loss_train:9.40, Loss_test:11.15\n",
      "epoch:638, Loss_train:9.39, Loss_test:11.14\n",
      "epoch:639, Loss_train:9.39, Loss_test:11.14\n",
      "epoch:640, Loss_train:9.38, Loss_test:11.14\n",
      "epoch:641, Loss_train:9.38, Loss_test:11.14\n",
      "epoch:642, Loss_train:9.37, Loss_test:11.14\n",
      "epoch:643, Loss_train:9.37, Loss_test:11.14\n",
      "epoch:644, Loss_train:9.36, Loss_test:11.14\n",
      "epoch:645, Loss_train:9.36, Loss_test:11.13\n",
      "epoch:646, Loss_train:9.36, Loss_test:11.13\n",
      "epoch:647, Loss_train:9.35, Loss_test:11.13\n",
      "epoch:648, Loss_train:9.35, Loss_test:11.13\n",
      "epoch:649, Loss_train:9.34, Loss_test:11.13\n",
      "epoch:650, Loss_train:9.34, Loss_test:11.12\n",
      "epoch:651, Loss_train:9.33, Loss_test:11.12\n",
      "epoch:652, Loss_train:9.33, Loss_test:11.12\n",
      "epoch:653, Loss_train:9.33, Loss_test:11.12\n",
      "epoch:654, Loss_train:9.32, Loss_test:11.12\n",
      "epoch:655, Loss_train:9.32, Loss_test:11.12\n",
      "epoch:656, Loss_train:9.31, Loss_test:11.12\n",
      "epoch:657, Loss_train:9.31, Loss_test:11.11\n",
      "epoch:658, Loss_train:9.30, Loss_test:11.11\n",
      "epoch:659, Loss_train:9.30, Loss_test:11.11\n",
      "epoch:660, Loss_train:9.30, Loss_test:11.11\n",
      "epoch:661, Loss_train:9.29, Loss_test:11.11\n",
      "epoch:662, Loss_train:9.29, Loss_test:11.10\n",
      "epoch:663, Loss_train:9.28, Loss_test:11.10\n",
      "epoch:664, Loss_train:9.28, Loss_test:11.10\n",
      "epoch:665, Loss_train:9.27, Loss_test:11.10\n",
      "epoch:666, Loss_train:9.27, Loss_test:11.10\n",
      "epoch:667, Loss_train:9.27, Loss_test:11.10\n",
      "epoch:668, Loss_train:9.26, Loss_test:11.09\n",
      "epoch:669, Loss_train:9.26, Loss_test:11.09\n",
      "epoch:670, Loss_train:9.25, Loss_test:11.09\n",
      "epoch:671, Loss_train:9.25, Loss_test:11.09\n",
      "epoch:672, Loss_train:9.25, Loss_test:11.09\n",
      "epoch:673, Loss_train:9.24, Loss_test:11.08\n",
      "epoch:674, Loss_train:9.24, Loss_test:11.08\n",
      "epoch:675, Loss_train:9.24, Loss_test:11.08\n",
      "epoch:676, Loss_train:9.23, Loss_test:11.08\n",
      "epoch:677, Loss_train:9.23, Loss_test:11.08\n",
      "epoch:678, Loss_train:9.22, Loss_test:11.08\n",
      "epoch:679, Loss_train:9.22, Loss_test:11.07\n",
      "epoch:680, Loss_train:9.22, Loss_test:11.07\n",
      "epoch:681, Loss_train:9.21, Loss_test:11.07\n",
      "epoch:682, Loss_train:9.21, Loss_test:11.07\n",
      "epoch:683, Loss_train:9.20, Loss_test:11.07\n",
      "epoch:684, Loss_train:9.20, Loss_test:11.07\n",
      "epoch:685, Loss_train:9.20, Loss_test:11.06\n",
      "epoch:686, Loss_train:9.19, Loss_test:11.06\n",
      "epoch:687, Loss_train:9.19, Loss_test:11.06\n",
      "epoch:688, Loss_train:9.18, Loss_test:11.06\n",
      "epoch:689, Loss_train:9.18, Loss_test:11.06\n",
      "epoch:690, Loss_train:9.18, Loss_test:11.05\n",
      "epoch:691, Loss_train:9.17, Loss_test:11.05\n",
      "epoch:692, Loss_train:9.17, Loss_test:11.05\n",
      "epoch:693, Loss_train:9.17, Loss_test:11.05\n",
      "epoch:694, Loss_train:9.16, Loss_test:11.05\n",
      "epoch:695, Loss_train:9.16, Loss_test:11.05\n",
      "epoch:696, Loss_train:9.15, Loss_test:11.04\n",
      "epoch:697, Loss_train:9.15, Loss_test:11.04\n",
      "epoch:698, Loss_train:9.15, Loss_test:11.04\n",
      "epoch:699, Loss_train:9.14, Loss_test:11.04\n",
      "epoch:700, Loss_train:9.14, Loss_test:11.04\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    \n",
    "    #Batch 학습\n",
    "    for idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        y_pred = y_pred.reshape(-1)\n",
    "        loss_train = criterion(y_pred, y_batch)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Validation\n",
    "        y_test_pred = model(x_test_tensor)\n",
    "        y_test_pred = y_test_pred.reshape(-1)\n",
    "        loss_test = criterion(y_test_pred, y_test_tensor)\n",
    "\n",
    "    train_loss.append(loss_train.item())\n",
    "    test_loss.append(loss_test.item())\n",
    "    print(\"epoch:{}, Loss_train:{:.2f}, Loss_test:{:.2f}\".format( epoch, train_loss[-1], test_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss 값 plot\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(test_loss, label='test')\n",
    "plt.title('Model loss')\n",
    "plt.legend(loc= 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <Scaling>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
