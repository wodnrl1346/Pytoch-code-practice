{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis(감정분석)\n",
    "영화 리뷰 글을 TorchText를 사용해서   \n",
    "긍정, 부정 평가를 다루는 모델을 만들어 본다\n"
   ]
  },
  {
   "attachments": {
    "sentiment1.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAACMCAYAAAAdt4G5AAAWoklEQVR4Xu2dX4gVZR/Hf4alW0bBGhHGUt1sRbElXRi73kgqbwRdRO7FKkkSJkuGVO8rwWpqBf2xJWWtpbxSLzYKAi/KG2/cpav+GLylF4GElf0xhLI1yvblGd9nnZ2dc86c852z55k5nwGp3X2+zzzz+f3Ob77zzDNz5k1NTU0ZGwQgAAEIQAACEEghMA+jQF5AAAIQgAAEIFCJAEaB3IAABCAAAQhAoCIBjALJAQEIQAACEIAARoEcyE7g0KFDduzYMRseHraOjo7sQloa7PQkgKHGEH4aP9SzCTCjQFbMIkChaTwpYNc4O6+EocYQfho/1BgFciADAQpNBkgVmsCucXYYBZ2d64EczIcjvVwmwIwC2VBxRmHp0qW2cePG6O89PT02NjZm3d3dEKtCwBdp2DWeJjBsnF3cKJCDGkfUGAVyoMbJbu3atbZz504bGhqyyclJ27JlS6Rg3UL11HEnOdhpHy8Ywk8jgDpvAswo5E20BP25Qn3gwIFoCrOzszM6oomJCRscHGRWoUZ8Yad/AGCoMYSfxg/1bAIYBbKi4q2H+OxBJaPgZxvWrVtnvb29bU8z7f5wkt3Jkyetv7/fjh8/HvHyMzdtD+//ALIwPHv2rA0MDNiRI0ci1cGDB6Of2dLXKFQz+u5vu3btmnFhAEcIxAlgFMiHho2CNwmjo6M2Pj6OUaiwkCxepBcvXhyd0NwtHWes/AnPGS1OdJdSsZZR6Orqim6FLV++PGLmjdfIyAg5mIFffJ2Rzz/P3c8gUhYhgFEgB6oSqFWoXaHxxXnZsmV26tSp6RNfu6PNwi7JyF3Nuc2ZB7baRiG5oNYbVm8c2p1hPTno2h4+fNjOnTvHjEK7J06V42dGgeRoaEbh9OnTkc69kCl+hdzuOOsp0p4VRmFm1tTL0JnWzZs32549e3gqp44ZBcdt//79tnLlStu9ezdGod2LF0aBDKiHQD2F2k9d+qn0evZTxrb1sHPHz7T57CzIyjB+64t1Hpc5ZuXnDOqKFSsiIWsUyliN8jsmZhTyY1manrIWGnfAGIXGr4Y9O7dWgdsO9Z/ovMIbhiVLlsAx44yCWzdz9OjRiBeLGUtTupt2IBiFpqFtj44xCo3FGZPQGLdKKk522Xk6Y7V9+3bbsGFDdKsGdtnZtWtLjEK7Rj6n48Yo1A+SJx3qZ1ZLwWuLaxG6/Pfk47n+L7x9NTvDdmuJUWi3iOd8vBiF+oAyTV4fr7TWyaccMF4aU2YUNH7toMYotEOUm3iMGIX64Fa6mnPfqcHrsbOzTL5wicWM2dklW2IUGmfXLkqMghjprz/qF3tovvyOf401fycN7gF+DYL7vwx+8NMIaGr/pXFaL81VuxfCsWkEMAoaP3OF+o6+f4u9NE/+9firFrpRgF/j8Sf/GmfnlPDT+PmZMK2X5qndGzwxCjpfjILIkEKjAYQf/DQCmpr80/hhFDR+RVFjFMRIUWg0gPCDn0ZAU5N/Gj+MgsavKGqMghgpCo0GEH7w0whoavJP44dR0PgVRY1RECNFodEAwg9+GgFNTf5p/DAKGr+iqDEKYqQoNBpA+MFPI6CpyT+NH0ZB41cUNUZBjBSFRgMIP/hpBDQ1+afxwyho/IqixiiIkaLQaADhBz+NgKYm/zR+GAWNX1HUGAUxUhQaDSD84KcR0NTkn8YPo6DxK4oaoyBGikKjAYQf/DQCmpr80/hhFDR+RVFjFMRIUWg0gPCDn0ZAU5N/Gj+MgsavKGqMghgpCo0GEH7w0whoavJP44dR0PgVRd2QUeC73y+Hl0KjpTr84KcR0NTkn8YPo6DxK4oaoyBGikKjAYQf/DQCmpr80/hhFDR+eaknJyct/gVYeX9tPUZBjBSFRgMIP/hpBDQ1+afxwyho/PJQJ02C7zNPsyAZhaVLl5r/PvKenh4bGxuz7u7uPI49iD5+//1327Nnjz322GO2ZMmS1DFRaCqHCn5aGsMPfq38mvi//vrL3n//fXv00Udt/vz5qcHAKFTO0Sz8tAy/pJ6YmLC+vj5bvXq1uWUBv/zyi/X390d/y+uc3LBRWLt2re3cudOGhobMOxo3sOHhYevo6Mjj+IPo49Zbb7Xvv//eHnroocg0JA0DRqF6mOCnpTH84HfHv8Y0CILa1bszZ87Y1q1b7aWXXprVE0ahOtxa/ITQTEt37dpl27Ztmz4fuz/43x08eNAGBgbk3TRsFA4cOBC5l87OzmlXMzg4mJuDkY8spw4++ugjc6bo119/tYULF9qDDz44wzBgFKqDhp+WiPCDXyuNgptRWL9+fRSE8+fPRyekHTt2TAcFo1A9P2vx07L7kjrNFKSZB2VfDRuFY8eOzZg9cNMfSaPgB+sGOD4+br29vXWP9eeff7affvqpbl2egocffti++eab6S69Ydi9e7dNfv0fu6Pv33nuLte+3NTlP13bcu2z3s7gVy+xme3hBz+NwGX1FVdcEf3g/pv13913320//PBDpLvmmmvswoULkVl49tlnbfPmzdF5INTNLfBzY8xry5vfggULpKHF1yfEZw/cRXx81l/aiZk1zSg44+CMghvwiRMnpv/fz0BkHfi+ffvM/WvldvbsWXOG5eLFi9PDWLRokT3xxBP2xMrvgjcKjz7331biM/hp+OEHP43AZfVdd91lX375pf3zzz+Z/7mTkTMHTuM3d7HkTkTuxBm6UXDnory2vPm988470tAKbxScSXBbfA3DunXrGppVkEjmIL7qqqvMLUxx2w033GA333yzvfrqq/bAAw8Ytx5qA4ZfbUbVWsAPfhoBTX3ttdeaW1jrNrf+7Morr7QXX3zRNm3aFM0ih24URkdHNQCiuhq/SotEs+6yklEoxK2Hrq6u6JnO5cuXRwsp/MH4n7NCCKGd+0C4RZvXX3/9DIPgx4ZRqB4l+GlZDD/4tXKNwptvvmnPP/+8TU1NRSbh5Zdfnn7SzUWGNQrV87MWPy27L6mDXsxYbY2CNwrxGQR3MLfddlsuKzDzgJu1jxtvvNFuuukme/3116MZhOSGUahOEn5ZMy29Hfzg10qjcN1110UzCK+88opt2LBhVjAwCtXzsxY/LbsvqZOPR7rfuQt097RKyx+PbBej8NVXX9mdd95ZMZ4YheqpDj+tFMAPfq00Cp988ondf//9FYOAUaien7X4adl9SR3sC5dqHVzyVkORbz3UOlaMQi1C1f8OP/hpBDQ1+afxwyho/PJSB/kK5ywHF7/V4A+iqIsZqx0vhSZLNlRuAz/4aQQ0Nfmn8cMoaPyKom7o8cgsB5fX45FZ9tXKNhQajT784KcR0NTkn8YPo6DxK4q6aUbBAcjjhUuhg6TQaBGCH/w0Apqa/NP4YRQ0fkVRN9UoFAWCMk4KjULPeA+Fhg9+8BMJaHKMgsavKGqMghgpjIIGEH7w0whoavJP44dR0PgVRY1RECNFodEAwg9+GgFNTf5p/DAKGr+iqDEKYqQoNBpA+MFPI6CpyT+NH0ZB41cUNUZBjBSFRgMIP/hpBDQ1+afxwyho/IqixiiIkaLQaADhBz+NgKYm/zR+GAWNX1HUGAUxUhQaDSD84KcR0NTkn8YPo6DxK4oaoyBGikKjAYQf/DQCmpr80/hhFDR+RVFjFMRIUWg0gPCDn0ZAU5N/Gj+MgsavKGqMghgpCo0GEH7w0whoavJP44dR0PgVRY1RECNFodEAwg9+GgFNTf5p/DAKGr+iqDEKYqRcoQl9a+X32ddiA79ahKr/HX7w0whoamcUQt9GR0dDH2Lw48MoBBSiixcv2tKlS+3TTz+1+fPnBzSyYgwFflqc4Ac/jQDqshLAKAQU2Weeecb27dtnmzZtsjfeeCOgkRVjKPDT4gQ/+GkEUJeVAEYhkMj++eeftmjRIvv777+j2YTffvvNFi5cGMjowh8G/LQYwQ9+GgHUZSaAUQgkuu5qbmRkxFzBdgaBWYX6AgO/+nglW8MPfhoB1GUmgFEIILrxqzk/HGYVsgcGftlZpbWEH/w0AqjLTgCjEECE41dzCxYsYFahzpjAr05giebwg59GAHXZCWAUWhxhdzV39dVXW0dHh9177702Pj5ufX199tlnn9mFCxfs/PnzrFWoEiP4aQkMP/hpBFC3AwGMQoujPDExYU8//bS9++67ds8999i8efNsamrKvvjiC3v88cdt79691tvb2+JRhrt7+GmxgR/8NAKo24EARiGwKHujENiwCjMc+Gmhgh/8NAKoy0gAoxBYVCnUWkDgBz+NgKYm/zR+qMMkgFEILC4UGi0g8IOfRkBTk38aP9RhEsAoBBYXCo0WEPjBTyOgqck/jR/qMAlgFAKLC4VGCwj84KcR0NTkn8YPdZgEMAqBxYVCowUEfvDTCGhq8k/jhzpMAhiFwOJCodECAj/4aQQ0Nfmn8UMdJgGMQmBxodBoAYEf/DQCmpr80/ihDpMARiGwuFBotIDAD34aAU1N/mn8UIdJAKMQWFwoNFpA4Ac/jYCmJv80fqjDJIBRCCwuFBotIPCDn0ZAU5N/Gj/UYRLAKAQWFwqNFhD4wU8joKnJP40f6jAJYBQCiwuFRgsI/OCnEdDU5J/GD3WYBDAKgcWFQqMFBH7w0whoavJP44c6TAIYhcDiQqHRAgI/+GkENDX5p/FDHSYBjEJgcaHQaAGBH/w0Apqa/NP4oQ6TAEYhsLhQaLSAwA9+GgFNTf5p/FCHSQCjEFhcKDRaQOAHP42Apib/NH6owySAUQgsLhQaLSDwg59GQFOTfxo/1GESwCgEFhcKjRYQ+MFPI6CpyT+NH+owCWAUAosLhUYLCPzgpxHQ1OSfxg91mAQwCoHFhUKjBQR+8NMIaGryT+OHOkwCGIXA4kKh0QICP/hpBDQ1+afxQx0mAYxCYHGh0GgBgR/8NAKamvzT+KEOkwBGIbC4UGi0gMAPfhoBTU3+afxQh0kAoxBYXCg0WkDgBz+NgKYm/zR+qMMkgFEILC4UGi0g8IOfRkBTk38aP9RhEsAoBBYXCo0WEPjBTyOgqck/jR/qMAlgFAKLC4VGCwj84KcR0NTkn8YPdZgEMAqBxYVCowUEfvDTCGhq8k/jhzpMAhiFwOJCodECAj/4aQQ0Nfmn8UMdJgGMQmBx2bFjh23fvj2wURVnOPDTYgU/+GkEUJeRAEahjFHlmCAAAQhAAAI5EcAo5ASSbiAAAQhAAAJlJIBRKGNUOSYIQAACEIBATgQwCjmBVLs5e/asDQwM2JkzZ2xsbMy6u7vVLgurP3TokK1du9Y2btxow8PD1tHRIR3LyZMnbf/+/ebuv6t9SQNpkXjXrl22YsUK6+3tNZ9nR44csfHx8eh3aZtj1t/fH/2pnfMxzsux2Lp1q33++ecW55d3vrYoTdgtBCoSwCgEkhwYhcuByLPwTkxMWF9fX26mI5B0yTSMyclJ27Jli42Ojk6bAoxCJnTTjZzJ2rZt2/TPb731ln344YcYhfow0rrgBDAKgQQQo4BRyDsVFaOQ91iK2p83CgcPHoxm/NK2PI1tUTkx7nITwCgEEl+MQrpRePLJJ239+vV2/Phx6+npmTUN7mcMvDrexhfweIh9wU9OKed1myOQdJpxi8GPafXq1bZ371576qmnpq+Ijx49On3FvHPnThsaGoqaV7r1kLzCjmtCOfY8xpHMD9eny623337bXnjhhaozCn/88cf0bcSRkRFzzNytCp+bixcvjv4e/10732rMI1700VwCGIXm8s3cO0ZhtlFIg+dOds4AdHZ2WtIkxE+Irs3HH38crXVIGoX77rsvuv/uzEd8i/edOXCBNkw70SWNQtrQvZFKMwpJk+D1ZTQLeRgFZwSS26pVq+zHH3+ckXtlyrtAPw4MSySAURAB5iXHKKQbBb/gzp+43Mnd/86fuJJtXE9+AV7aGoW06eQsU8x5xXqu+ql16yE+i+KP3//u22+/nbGYsaura9Z6B8+2zCe6ZF6krfFI3nrwMwrOKHgTFTe1yd+lzZTNVY6wHwhkIYBRyEJpDtpgFGYbhfiJrNoivPgJ0fUSL7xJoxAv4mlhLdPVcS2jEH/qIXmySxoFNzWenFGo9tTEHHxk5mQXqlHwjNLyl8/8nISQneRAAKOQA8Q8uqBo1G8U0tYgqEahTGsV8jYKadPxjneZmCU/y4pRiD/qnPb55jOfR+Wkj7kggFGYC8oZ9kHRqM8o3H777dMLwvwsQNp99WozCmW/Is7bKMTTODmLU1aWGIUMxYsmpSeAUQgkxBiF+oyCWzmefCGQn2FIu/UQv4+evB/v9uzfN1DtMbhAUiXzMOIn87SnPeq59RBfqe91af1nHlxBGmIUChIohtlUAhiFpuLN3jlGoT6jEJ9RSFJOMwq+jZt9WLNmTemfenDHm7Z2I+3xPtc2yxqFLLd6smd8MVpiFIoRJ0bZXAIYhebyzdw7RqE+o+BePRx/EsKbg/feey96L0B8ZiC+CC95m8I/IlnWlftJRopRiBsKH62ycvPHh1HIXMJoWGICGIUSB5dDgwAEIAABCKgEMAoqQfQQgAAEIACBEhPAKJQ4uBwaBCAAAQhAQCWAUVAJoocABCAAAQiUmABGocTB5dAgAAEIQAACKgGMgkoQPQQgAAEIQKDEBDAKJQ4uhwYBCEAAAhBQCWAUVILoIQABCEAAAiUmgFEocXA5NAhAAAIQgIBKAKOgEkQPAQhAAAIQKDEBjMIcB9e/UveRRx6xoaGhaO/+9c233HKLDQ8PW0dHxxyPqli7g6EWL/jBTyOAut0IYBRaEHH31ceDg4M2NjZm3d3d5t4n737nvnSns7OzBSMq3i5hqMUMfvDTCKBuJwIYhRZF25mD7777ztzMwqpVqyz+lb8tGlLhdgtDLWTwg59GAHW7EMAotCjS8W/1899omByK/5rgdevWmfu2RLaZBKoxjP/NqSoxbmem1fj522FHjhyJEMW/jbOdmcWPPctn2LV3szfOlDFjSOYUlQBGoYWRc8Xjgw8+mL4FER+KNwmjo6PMNlSJURpDf5Jza0CcwfI/O8M1MDDQwoiHt+s0fj73li9fHvHyJ8SRkREMayKE1T7DrqnPPff/GIXw8p8RZSOAUcjGKfdW7iqjr68v6nfjxo0zFjH6wrxs2TI7depUtOiRGYXZIajGMNnaFXS3+QWkuQe0gB1m5Zc0DgU81KYMOQs/Zw4OHz5s586dwyg0JQp0OhcEMApzQTmxD3+V4U7+a9assf7+fnvuueemr3ZPnz4dKdzTD+6KDqMwO0i1GGIUqid2Pfyccd28ebPt2bMnWnzLdnmmoNJn2DFy3Pbv328rV6603bt3YxRInMISwCi0IHTuKuPAgQPThcP9/Nprr826BZGcQm/BUIPdZVaGvmA7M8bU+eVwZuEXv/3FGo+ZH4Us/Nws1ooVKyIhaxSCLSUMLAMBjEIGSHk28dOV8cVhviC72wzx+5gYhXTyjTB0V37cdrjEsx5+rr3PzyVLlsAwI78TJ07Y0aNHI14sZsyzgtJXKwhgFFpBPeM+MQoZQVVoFp9exyRoLDnZZefnjNX27dttw4YN0a0a2GVnR8swCWAUwoxLNCqMQuPB4UmHxtmlKd1M17Fjx3hzaAasyUdzvaSnpyf1CacMXdIEAi0lgFFoKf7qO8coNBYcpsob4+ZVyaccMF0aT2YUNH6oW08Ao9D6GFQcAUahseBUuqJLPobaWO/toUq+cInFjI3HHaPQODuUYRDAKIQRB0YBAQhAAAIQCJLA/wCRyXm2kmR0cwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sentiment1.png](attachment:sentiment1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-사전 라이브러리 설치 필요  \n",
    "-TorchText 설치하기  \n",
    "    pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "num_batch = 128\n",
    "num_layers = 1\n",
    "hidden_size = 128\n",
    "embedding_dim = 128\n",
    "class_num = 2\n",
    "dropout = 0.2\n",
    "learning_rate = 0.01\n",
    "epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB dataset  \n",
    " -영문으로 된 5만건의 영화 리뷰 텍스트 데이터 셋  \n",
    " -Label --> pos(2), neg(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-TorchText의 Field 클래스는 데이터를 어떻게 처리할 것인지 정하는 객체이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-TorchText에서 제공하는 IMDB 데이셋을 다운로드 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\imdb\\aclImdb_v1.tar.gz: 100%|███████████████████████████████████████████████| 84.1M/84.1M [00:37<00:00, 2.22MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMDB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEXT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLABEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trainset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of test samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(testset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep1\\lib\\site-packages\\torchtext\\datasets\\imdb.py:53\u001b[0m, in \u001b[0;36mIMDB.splits\u001b[1;34m(cls, text_field, label_field, root, train, test, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplits\u001b[39m(\u001b[38;5;28mcls\u001b[39m, text_field, label_field, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     41\u001b[0m            train\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124;03m\"\"\"Create dataset objects for splits of the IMDB dataset.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m            Dataset.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMDB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_field\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep1\\lib\\site-packages\\torchtext\\data\\dataset.py:76\u001b[0m, in \u001b[0;36mDataset.splits\u001b[1;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"Create Dataset objects for multiple splits of a dataset.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    test splits in that order, if provided.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     78\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, train), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     79\u001b[0m val_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m validation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     80\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, validation), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep1\\lib\\site-packages\\torchtext\\data\\dataset.py:193\u001b[0m, in \u001b[0;36mDataset.download\u001b[1;34m(cls, root, check)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(zpath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr:gz\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[0;32m    192\u001b[0m         dirs \u001b[38;5;241m=\u001b[39m [member \u001b[38;5;28;01mfor\u001b[39;00m member \u001b[38;5;129;01min\u001b[39;00m tar\u001b[38;5;241m.\u001b[39mgetmembers()]\n\u001b[1;32m--> 193\u001b[0m         \u001b[43mtar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmembers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(zpath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m gz:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep1\\lib\\tarfile.py:2028\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[1;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[0;32m   2026\u001b[0m         tarinfo\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0o700\u001b[39m\n\u001b[0;32m   2027\u001b[0m     \u001b[38;5;66;03m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[1;32m-> 2028\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[0;32m   2032\u001b[0m directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep1\\lib\\tarfile.py:2069\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[1;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2066\u001b[0m     tarinfo\u001b[38;5;241m.\u001b[39m_link_target \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, tarinfo\u001b[38;5;241m.\u001b[39mlinkname)\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2071\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrorlevel \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep1\\lib\\tarfile.py:2159\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[1;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39missym():\n\u001b[0;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchmod(tarinfo, targetpath)\n\u001b[1;32m-> 2159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep1\\lib\\tarfile.py:2292\u001b[0m, in \u001b[0;36mTarFile.utime\u001b[1;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[0;32m   2290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2292\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExtractError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not change modification time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(f'Number of training samples: {len(trainset)}')\n",
    "print(f'Number of test samples: {len(testset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data, Validation data로 나누기\n",
    "train_data, val_data = trainset.split(split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "#Batchfy\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train_data, val_data, testset), batch_size=num_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (pad)와 (unk) 토큰이 추가 되어 단어장이 생성된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 46159\n",
      "Unique tokens in LABEL vocabulary: 3\n"
     ]
    }
   ],
   "source": [
    "# 단어장 생성\n",
    "TEXT.build_vocab(trainset, min_freq=5)   # min_freq: 입력값만큼 반복되는 단어만 단어장 생성\n",
    "LABEL.build_vocab(trainset)                                 # 나머지는 <unk> 처리\n",
    "\\\n",
    "voca_num = len(TEXT.vocab)\n",
    "\n",
    "print(f'Unique tokens in TEXT vocabulary: { len(TEXT.vocab)}')\n",
    "print(f'Unique tokens in LABEL vocabulary: { len(LABEL.vocab) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 322198), ('a', 159953), ('and', 158572), ('of', 144462), ('to', 133967), ('is', 104171), ('in', 90527), ('i', 70480), ('this', 69714), ('that', 66292), ('it', 65505), ('/><br', 50935), ('was', 47024), ('as', 45102), ('for', 42843), ('with', 42729), ('but', 39764), ('on', 31619), ('movie', 30887), ('his', 29059)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20)) #자주 등장하는 단어와 횟수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.stoi) # 어휘사전(dict), 키값은 voca이고 value값은 인덱스 값\n",
    "                       # stoi(string to int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f060821d290>>, {'<unk>': 0, 'neg': 1, 'pos': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# [2] Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, n_vocab,  hidden_dim, embed_dim, n_classes, n_layers, dropout):\n",
    "        super(SeqModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, embeb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, n_classes)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        outputs, status = self.gru(x)\n",
    "        \n",
    "        outputs = outputs[:,-1,:]\n",
    "        outputs = self.tanh(outputs)\n",
    "        outputs = self.Linear(outputs)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeqModel(voca_num, hidden_size, embedding_dim, class_num, num_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in val_iter:\n",
    "        x_test = batch.text.to(device)\n",
    "        target = batch.label.to(device)\n",
    "        target.data.sub_(1)  # label value (1,2) --> (0,1)로 변환\n",
    "        \n",
    "        output = model(x_test)\n",
    "        \n",
    "        outputs_softmax = F.softmax(output, dim=1)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pred = torch.argmax(outputs_softmax.data, dim=1)\n",
    "        corrects += (pred == target.data).sum().item()\n",
    "        \n",
    "    length = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / length\n",
    "    avg_accuracy = (corrects/length)*100\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 , Loss:0.0031, Accuracy:82.60 \n",
      "Epoch:2 , Loss:0.0029, Accuracy:84.04 \n",
      "Epoch:3 , Loss:0.0028, Accuracy:85.70 \n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "for i in range(1, epoch+1):\n",
    "    model.train()\n",
    "    \n",
    "    for b, batch in enumerate(train_iter):\n",
    "\n",
    "        x_train = batch.text.to(device)\n",
    "        target = batch.label.to(device)\n",
    "        target.data.sub_(1)  # label value (1,2) --> (0,1)로 변환\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x_train)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "    print( f'Epoch:{i} , Loss:{val_loss:.4f}, Accuracy:{val_accuracy:.2f} ')\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.00, test_acc: 83.79\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iter)\n",
    "print(f'test_loss: {test_loss :.2f}, test_acc: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference[1]](https://github.com/bentrevett/pytorch-sentiment-analysis)  \n",
    "[Reference[2]](https://github.com/keon/3-min-pytorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
